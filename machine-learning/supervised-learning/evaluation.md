# Evaluation

超参数的调优是一个相当复杂与繁琐的任务。在模型原型设计阶段，需要尝试不同的模型、不同的超参数意见不同的特征集，我们需要寻找一个最优的超参数，因此需要使用相关的搜索算法去寻找，如格搜索（grid search）、随机搜索（random search）以及启发式搜索（smart search）等。这些搜索算法是从超参数空间中寻找一个最优的值。

当模型使用离线数据训练好并满足要求后，就需要将模型使用新的在线数据进行上线测试，这就是所谓的在线测试。在线测试不同于离线测试，有着不同的测试方法以及评价指标。最常见的便是A/B testing，它是一种统计假设检验方法。不过，在进行A/B testing的时候，会遇到很多陷阱与挑战，具体会在本文后面进行详细介绍。另一个相对使用较小的在线测试方法是multiarmed bandits。在某些情况下，它比A/B testing的效果要好。后面会进行具体讲解。

#### 评价指标\(Evaluation metrics\) <a id="&#x8BC4;&#x4EF7;&#x6307;&#x6807;evaluation-metrics"></a>

评价指标是机器学习任务中非常重要的一环。不同的机器学习任务有着不同的评价指标，同时同一种机器学习任务也有着不同的评价指标，每个指标的着重点不一样。如分类（classification）、回归（regression）、排序（ranking）、聚类（clustering）、热门主题模型（topic modeling）、推荐（recommendation）等。并且很多指标可以对多种不同的机器学习模型进行评价，如精确率－召回率（precision-recall），可以用在分类、推荐、排序等中。像分类、回归、排序都是监督式机器学习，本文的重点便是监督式机器学习的一些评价指标。








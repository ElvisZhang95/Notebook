# Evaluation

{% embed url="https://blog.csdn.net/heyongluoyao8/article/details/49408319" %}

超参数的调优是一个相当复杂与繁琐的任务。在模型原型设计阶段，需要尝试不同的模型、不同的超参数意见不同的特征集，我们需要寻找一个最优的超参数，因此需要使用相关的搜索算法去寻找，如格搜索（grid search）、随机搜索（random search）以及启发式搜索（smart search）等。这些搜索算法是从超参数空间中寻找一个最优的值。

当模型使用离线数据训练好并满足要求后，就需要将模型使用新的在线数据进行上线测试，这就是所谓的在线测试。在线测试不同于离线测试，有着不同的测试方法以及评价指标。最常见的便是A/B testing，它是一种统计假设检验方法。不过，在进行A/B testing的时候，会遇到很多陷阱与挑战，具体会在本文后面进行详细介绍。另一个相对使用较小的在线测试方法是multiarmed bandits。在某些情况下，它比A/B testing的效果要好。后面会进行具体讲解。

#### 评价指标\(Evaluation metrics\) <a id="&#x8BC4;&#x4EF7;&#x6307;&#x6807;evaluation-metrics"></a>

评价指标是机器学习任务中非常重要的一环。不同的机器学习任务有着不同的评价指标，同时同一种机器学习任务也有着不同的评价指标，每个指标的着重点不一样。如分类（classification）、回归（regression）、排序（ranking）、聚类（clustering）、热门主题模型（topic modeling）、推荐（recommendation）等。并且很多指标可以对多种不同的机器学习模型进行评价，如精确率－召回率（precision-recall），可以用在分类、推荐、排序等中。像分类、回归、排序都是监督式机器学习，本文的重点便是监督式机器学习的一些评价指标。

**准确率\(Accuracy\)**

  准确率是指在分类中，使用测试集对模型进行分类，分类正确的记录个数占总记录个数的比例：

  $$accuracy = \frac{n_{correct}}{n_{total}}$$   
  准确率看起来非常简单。然而，准确率评价指标没有对不同类别进行区分，即其平等对待每个类别。但是这种评价有时是不够的，比如有时要看类别0与类别1下分类错误的各自个数，因为不同类别下分类错误的代价不同，即对不同类别的偏向不同，比如有句话为“宁可错杀一万，不可放过一千“就是这个道理，例如在病患诊断中，诊断患有癌症实际上却未患癌症（False Positive）与诊断未患有癌症的实际上却患有癌症（False Negative）的这两种情况的重要性不一样。。另一个原因是，可能数据分布不平衡，即有的类别下的样本过多，有的类别下的样本个数过少，两类个数相差较大。这样，样本占大部分的类别主导了准确率的计算，为了解决这个问题，对准确率进行改进，得到平均准确率。

**平均准确率\(Average Per-class Accuracy\)**

  为了应对每个类别下样本的个数不一样的情况，对准确率进行变种，计算每个类别下的准确率，然后再计算它们的平均值。举例，类别0的准确率为80%，类别1下的准确率为97.5%，那么平均准确率为\(80%+97.5%\)/2=88.75%。因为每个类别下类别的样本个数不一样，即计算每个类别的准确率时，分母不一样，则平均准确率不等于准确率，如果每个类别下的样本个数一样，则平均准确率与准确率相等。   
  平均准确率也有自己的缺点，比如，如果存在某个类别，类别的样本个数很少，那么使用测试集进行测试时（如k-fold cross validation），可能造成该类别准确率的方差过大，意味着该类别的准确率可靠性不强。

**对数损失函数\(Log-loss\) = logistic loss**

  在分类输出中，若输出不再是0-1，而是实数值，即属于每个类别的概率，那么可以使用Log-loss对分类结果进行评价。这个输出概率表示该记录所属的其对应的类别的置信度。比如如果样本本属于类别0，但是分类器则输出其属于类别1的概率为0.51，那么这种情况认为分类器出错了。该概率接近了分类器的分类的边界概率0.5。Log-loss是一个软的分类准确率度量方法，使用概率来表示其所属的类别的置信度。Log-loss具体的数学表达式为： 

$$log\_loss=-\frac{1}{N}\sum_{i=1}^{N}y_i log p_i + (1-y_i)log(1-p_i)$$ 

* **cross entropy和KL-divergence作为目标函数效果是一样的，从数学上来说相差一个常数。**
* **logistic loss 是cross entropy的一个特例**

#### 1. cross entropy和KL-divergence <a id="1-cross-entropy&#x548C;kl-divergence"></a>

假设两个概率分布 $$p(x)$$ 和 $$q(x)$$ ， $$H(p,q)$$ 为cross entropy， $$D_{KL}(p|q)$$ 为 KL divergence。

**交叉熵的定义：** $$H(p,q)=-\sum_xp(x) \log {q(x)}$$ 

**KL divergence的定义：** $$D_{KL}(p|q)=\sum _xp(x)\log  \frac{ p(x)}  {q(x)}$$ 

**推导：**

 ****$$\begin{align}  D_{KL}(p|q) & = \sum _xp(x)\log  \frac{ p(x)}  {q(x)} \\  & = \sum_x(p(x)\log p(x)-p(x)\log q(x))\\   & =-H(p)-\sum_xp(x) \log q(x)\\  & = -H(p)+H(p,q) \\  \end{align}$$ 

也就是说，cross entropy也可以定义为： ****$$H(p,q)=D_{KL}(p|q)+H(p)$$ 

**直观来说，由于p\(x\)是已知的分布，H\(p\)是个常数，cross entropy和KL divergence之间相差一个常数。**

#### 2. logistic loss 和cross entropy <a id="2-logistic-loss-&#x548C;cross-entropy"></a>

假设 $$p \in \{y,1-y\}$$ ， $$q \in \{ \hat y,1-\hat y \}$$ ， cross entropy可以写为logistic loss：

$$H(p,q)=-\sum_xp(x) \log {q(x)}=-y\log\hat y-(1-y)\log(1-\hat y)$$ 



**精确率-召回率\(Precision-Recall\)**

  精确率-召回率其实是两个评价指标。但是它们一般都是同时使用。精确率是指分类器分类正确的正样本的个数占该分类器所有分类为正样本个数的比例。召回率是指分类器分类正确的正样本个数占所有的正样本个数的比例。

> 实际上非常简单，**精确率**是针对我们**预测结果**而言的，它表示的是预测为正的样本中有多少是真正的正样本。那么预测为正就有两种可能了，一种就是把正类预测为正类\(TP\)，另一种就是把负类预测为正类\(FP\)，也就是  
> ![P  = \frac{TP}{TP+FP}](https://www.zhihu.com/equation?tex=P++%3D+%5Cfrac%7BTP%7D%7BTP%2BFP%7D)  
> 而**召回率**是针对我们原来的**样本**而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类\(TP\)，另一种就是把原来的正类预测为负类\(FN\)。  
> ![R = \frac{TP}{TP+FN}](https://www.zhihu.com/equation?tex=R+%3D+%5Cfrac%7BTP%7D%7BTP%2BFN%7D)

其实就是分母不同，一个分母是预测为正的样本数，另一个是原来样本中所有的正样本数。  
  


**F1-score：**

  F1-score为精确率与召回率的调和平均值，它的值更接近于Precision与Recall中较小的值。即：

$$F1=\frac{2*precision*recall}{precision+recall}$$ 



**AUC\(Area under the Curve\(Receiver Operating Characteristic, ROC\)\)**

  AUC的全称是Area under the Curve，即曲线下的面积，这条曲线便是ROC曲线，全称为the Receiver Operating Characteristic曲线，它最开始使用是上世纪50年代的电信号分析中，在1978年的“Basic Principles of ROC Analysis ”开始流行起来。ROC曲线描述分类器的True Positive Rate（TPR，分类器分类正确的正样本个数占总正样本个数的比例）与False Positive Rate（FPR，分类器分类错误的负样本个数占总负样本个数的比例）之间的变化关系。

一般来说，如果ROC是光滑的，那么基本可以判断没有太大的overfitting（比如图中0.2到0.4可能就有问题，但是样本太少了），这个时候调模型可以只看AUC，面积越大一般认为模型越好。

![](../../.gitbook/assets/image%20%2829%29.png)

_**注意： AUC只是表示曲线下的面积 也有AUC of precision-Recall**_

再说PRC， precision recall curve。和ROC一样，先看平滑不平滑（蓝线明显好些），在看谁上谁下（同一测试集上），一般来说，上面的比下面的好（绿线比红线好）。F1（计算公式略）当P和R接近就也越大，一般会画连接\(0,0\)和\(1,1\)的线，线和PRC重合的地方的F1是这条线最大的F1（光滑的情况下），此时的F1对于PRC就好象AUC对于ROC一样。**一个数字比一条线更方便调模型。**  
  


![](../../.gitbook/assets/image%20%2811%29.png)

ROC和PR曲线可以用于衡量样本不平衡的情况。当Precision和recall的值差异很大时，你就不太能通过F1 Score来衡量了。



**混淆矩阵\(Confusion Matrix\)**

  混淆矩阵是对分类的结果进行详细描述的一个表，无论是分类正确还是错误，并且对不同的类别进行了区分，对于二分类则是一个2\*2的矩阵，对于n分类则是n\*n的矩阵。对于二分类，第一行是真实类别为“Positive”的记录个数（样本个数），第二行则是真实类别为“Negative”的记录个数，第一列是预测值为“Positive”的记录个数，第二列则是预测值为“Negative”的记录个数。如下表所示：

|  | Predicted as Positive | Predicted as Negative |
| :--- | :--- | :--- |
| Labeled as Positive | True Positive\(TP\) | False Negative\(FN\) |
| Labeled as Negative | False Positive\(FP\) | True Negative\(TN\) |

如上表，可以将结果分为四类：   
\* 真正\(True Positive, TP\)：被模型分类正确的正样本；   
\* 假负\(False Negative, FN\)：被模型分类错误的正样本；   
\* 假正\(False Positive, FP\)：被模型分类的负样本；   
\* 真负\(True Negative, TN\)：被模型分类正确的负样本；

进一步可以推出这些指标：

* 真正率\(True Positive Rate, TPR\)，又名灵敏度\(Sensitivity\)：分类正确的正样本个数占整个正样本个数的比例，即： $$TPR = \frac{TP}{TP+FN}$$ 
* 假负率\(False Negative Rate, FNR\)：分类错误的正样本的个数占正样本的个数的比例，即： $$FNR = \frac{FN}{TP+FN}$$ 
* 假正率\(False Positive Rate, FPR\)：分类错误的负样本个数占整个负样本个数的比例，即： $$FPR = \frac{FP}{FP+TN}$$ 
* 真负率\(True Negative Rate, TNR\)：分类正确的负样本的个数占负样本的个数的比例，即： $$TNR = \frac{TN}{FP+TN}$$ 

进一步，由混淆矩阵可以计算以下评价指标：准确率，精确率，召回率，F1-Score, ROC曲线





**回归评价指标**

  与分类不同的是，回归是对连续的实数值进行预测，即输出值是连续的实数值，而分类中是离散值。例如，给你历史股票价格，公司与市场的一些信息，需要你去预测将来一段时间内股票的价格走势。那么这个任务便是回归任务。对于回归模型的评价指标主要有以下几种：   
\* RMSE   
  回归模型中最常用的评价模型便是RMSE（root mean square error，平方根误差），其又被称为RMSD（root mean square deviation），其定义如下： $$RMSE = \sqrt{\frac{\sum_{i=0}^n(y_i-\hat{y_i})^2}{n}}$$ 

* Quantiles of Errors 

    为了改进RMSE的缺点，提高评价指标的鲁棒性，使用误差的分位数来代替，如中位数来代替平均数。假设100个数，最大的数再怎么改变，中位数也不会变，因此其对异常点具有鲁棒性。 

    在现实数据中，往往会存在异常点，并且模型可能对异常点拟合得并不好，因此提高评价指标的鲁棒性至关重要，于是可以使用中位数来替代平均数，如MAPE： 

其中， $$y_{i}$$ 是第i个样本的真实值， $$\hat{y_i}$$ 是第i个样本的预测值，n是样本的个数。该评价指标使用的便是欧式距离。 RMSE虽然广为使用，但是其存在一些缺点，因为它是使用平均误差，而平均值对异常点（outliers）较敏感，如果回归器对某个点的回归值很不理性，那么它的误差则较大，从而会对RMSE的值有较大影响，即平均值是非鲁棒的。

$$MAPE=median(|y_i-\hat{y_i}|/y_i)$$ 

MAPE是一个相对误差的中位数，当然也可以使用别的分位数。

* “Almost Crrect” Predictions 

    有时我们可以使用相对误差不超过设定的值来计算平均误差，如当 $$|y_i-\hat{y_i}|/y_i$$ 超过100%（具体的值要根据问题的实际情况）则认为其是一个异常点，，从而剔除这个异常点，将异常点剔除之后，再计算平均误差或者中位数误差来对模型进行评价。

  
**排序评价指标**

  排序任务指对对象集按照与输入的相关性进行排序并返回排序结果的过程。举例，我们在使用搜索引擎（如google，baidu）的时候，我们输入一个关键词或多个关键词，那么系统将按照相关性得分返回检索结果的页面。此时搜索引擎便是一个排序器。其实，排序也可以说是一个二分类问题。即将对象池中的对象分为与查询词相关的正类与不相关的负类。并且每一个对象都有一个得分，即其属于正类的置信度，然后按照这个置信度将正类进行排序并返回。   
  另一个与排序相关的例子便是个性化推荐引擎。个性化推荐引擎便是根据用户的历史行为信息或者元信息计算出每个用户当前有兴趣的项目，并为每个项目赋一个兴趣值，最好按照这个兴趣值进行排序，返回top n兴趣项目。 

$$precision = \frac{happy\ correct\ answers}{total\ items\ returned\ by\ ranker}$$ 

$$recall = \frac{happy\ correct\ answers}{total\ relevant\ items}$$ 

当我们改变top k中的k值时，便可以得到不同的精确率与召回率，那么我们可以通过改变k值而得到精确率曲线和召回率曲线。与ROC曲线一样，我们也需要一个定量的指标对其ROC曲线进行描述而来评价其对应的模型进行评价。可取多个k值，然后计算其评价的精确率与召回率。

NDCG 

在精确率与召回率中，返回集中每个项目的地位（权值）是一样，即位置k处的项目与位置1处的项目地位一样，但是实际情况应该是越排在前面的项目越相关，得分越高。NDCG（normalized discounted cumulative gain）指标便考虑了这种情况，在介绍NDCG之前，首先介绍一下CG\(cumulative gain与DCG\(discounted cumulative gain\)。CG是对排序返回的top k个项目的相关性（即得分）求和，而DCG在每个项目的得分乘上一个权值，该权值与位置成反方向（如成反比），即位置越近，权值越大。而NDCG则对每项的带权值得分先进行归一化，然后再求和。 _**在信息检索中或者那些对项目的返回位置关心的模型中经常使用DCG或NDCG。**_



模型验证，模型选择

**Hold-out Validation**

  Hold-out Validation较简单，它假设数据集中的每个数据点都是独立同分布的（i.i.d,independently and identically distributed）。因此我们只需要简单得将原数据集随机划分成两个部分，较大的部分作为训练集，用来训练数据，较小的部分作为验证集，用来对模型进行验证。   
  从计算的角度来说，Hold-out Validation是简单的并运行时间快的。缺点便是它是强假设的，缺乏有效的统计特征，并且验证数据集较小，那么其验证的结果则可靠性较低，同时也很难在单个数据集上计算方差信息与置信区间。因此如果需要使用 hold-out validation方法，则需要足够的数据以确保验证集数据足够而确保可靠的统计估计。

**Cross-Validation**

  Cross-Validation是另一种模型训练集与验证集的产生方法，即将数据集划分成多个小部分集合，如划分成k个部分，那么就变为了k-fold cross validation。依次使用其中的k-1个数据集对模型进行训练（每次使用k-1个不同的数据集），然后使用剩下的一个数据集对模型进行评价，计算评价指标值。接着重复前面的步骤，重复k次，得到k个评价指标值。最后计算这k个评价指标的平均值。其中k是一个超参数，我们可以尝试多个k，选择最好的平均评价指标值所对应的k为最终的k值。   
  另一个Cross-Validation的变种便是leave-one-out。该方法与k-fold cross validation方法类似，只是k等于数据集中样本的总数目，即每次使用n-1个数据点对模型进行训练，使用最好一个数据点对模型进行训练。重复n次，计算每次的评价指标值，最后得到平均评价指标值。该方法又称为n-fold cross validation。   
  当数据集较小时以致hold-out validation效果较差的时候，cross validation是一种非常有效的训练集-验证集产生方法。

**Bootstrapping和Jackknife**

  Bootstrapping是一种重采样技术，翻译成自助法。它通过采样技术从原始的单个数据集上产生多个新的数据集，每个新的数据集称为一个bootstrapped dataset，并且每个新的数据集大小与原始数据集大小相等。这样，每个新的数据集都可以用来对模型进行评价，从而可以得到多个评价值，进一步可以得到评价方差与置信区间。   
  Bootstrapping与Cross Validation交叉校验相关。Bootstrapping对原数据集进行采样生成一个新的数据集（ bootstrapped dataset）。不同的是，Bootstrapping假设每个数据点都服从均匀分布。它采用的是一种有放回的采样，即将原始数据集通过采样生成一个新的数据集时，每次采样都是有放回得采样，那么这样在新生成的数据集中，可能存在重复的数据点，并且可能会重复多次。   
  为什么使用有放回的采样？每一个样本都可以用一个真实的分布进行描述，但是该分布我们并不知道，我们只有一个数据集去推导该分布，因此我们只能用该数据集去估计得到一个经验分布。Bootstrap假设新的样本都是从该经验分布中得到的，即新的数据集服从该经验分布，并且分布一直不变。如果每次采样后不进行放回，那么这个经验分布会一直改变。因此需要进行有放回的采样。   
  显然采样后得到的新数据集中会包含同样的样本多次。如果重复采样n次，那么原数据集中的样本出现在新的数据集中的概率为1−1/e≈63.2%1−1/e≈63.2%，用另外一种讲法，原数据集中有约2/3的数据会在新数据集中出现，并且有的会重复多次。   
  在对模型进行校验时，可以使用新生成的数据集（ bootstrapped dataset）对模型进行训练，使用未被采样的样本集来对模型进行验证。这种方式类似交叉校验。   
  Jackknife翻译成刀切法。Jackknife即从原始进行不放回采样m\(m

**Cross Validation与Bootstrap的比较**

**相同之处**，两者都是resampling的方法，都是在数据集较小的时候常用的方法。**不同之处**，我觉得主要存在于亮点：  
**其一**，两者的目的不同。CV主要用于model selection，例如KNN中选多大的K，使得估计的test error比较小。而Bootstrap主要用来看选定的model的uncertainty，例如参数的标准差多大。  
**其二**，两者的resample方法不同。在k fold CV中，把原始数据集分成k等分（各等分之间没交集），每一次validation中，把其中一份作为validation set，剩余的作为training set。而在Bootstrap中，并不区分training和validation set，并且在resample中，是允许replacement的，即同一个sample可以重复出现。  
  
 **注意：模型验证与测试不同**

在原型设计阶段中，需要进行模型选择，即需要对多个候选模型在一个或多个验证集上进行性能评价。当在模型训练与验证确定了合适的模型类型（如分类中是采用决策树还是svm等）以及最优的超参数（如特征的个数）后，需要使用全部可利用的数据\(包括前面对模型进行验证的验证集\)对模型进行训练，训练出的模型便是最终的模型，即上线生产的模型。 模型测试则发生在模型的原型设计之后，即包含在上线阶段又包含在离线监视（监测分布漂移 distribution drift）阶段。 不要将训练数据、验证数据与测试数据相混淆。模型的训练、验证与测试应该使用不同的数据集，如果验证数据集、测试数据集与训练数据集有重叠部分，那么会导致模型的泛化能力差。



_**参数搜索方法**_

**格搜索\(Grid Search\)**

  [顾名思义](https://www.baidu.com/s?wd=%E9%A1%BE%E5%90%8D%E6%80%9D%E4%B9%89&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)，格搜索便是将超参数的取值范围划分成一个个格子，然后对每一个格子所对应的值进行评估，选择评估结果最好的格子所对应的超参数值。例如，对于决策树叶子节点个数这一超参数，可以将值划分为这些格子：10, 20, 30, …, 100, …；又如正则化因子这一超参数，一般使用指数值，那么可以划分为：1e-5, 1e-4 1e-3, …, 1。有时可以进行猜测对格子进行搜索去获得最优的超参数。如，当从第一个开始，发现效果较差，第二个好了一点，那么可以第三个可以取最后一个。格搜索较为简单并且可以进行并行化。

**随机搜索\(Random Search\)**

  在论文 “[Random Search for Hyper Parameter Optimization”](http://bit.ly/bergstra-bengio) \(Bergstra and Bengio\)中，已经验证了随机搜索是一个简单而有效的方法。它是格搜索的变种。相比于搜索整个格空间，随机搜索只对随机采样的那些格进行计算，然后在这中间选择一个最好的。因此随机搜索比格搜索的代价低。随机搜索有个缺点，即其可能找不到最优的点。但是前面的那篇论文已经证明，随机采样60个点的性能已经足够好了。从概率的角度来说，对于任何的分布的样本空间若存在最大值，那么随机采样60个点中的最大值位于整个样本空间top5%的值的集合中的概率达到95%。证明如下：   
  对于top%5的值，我们每次随机采样，得到top5%的值的概率为5%，没有得到top5%的值的概率为\(1-0.05\)，重复有放回地采样n次，那么至少有一次得到top5的值这件事发生的概率若要超过95%，则： 

$$1-(1-0.05)^n>=0.95 \Rightarrow n>=60$$ 

这表示我们只需要从所有候选格中随机采样60个格，便可以以95%的概率得到至少一个的top5%最优的格。因此随机搜索60个格进行计算便可以以很高的概率得到top%5最优的格。当最优格以及近似最优格的集合较大，加上机器学习模型对超参数一个近似最优值与最优值不会太敏感，因此，随机搜索算法便是有效的。由于随机搜索简单并且有效，一般是超参数调优的首选搜索算法。并且其容易并行化。

**智能搜索\(Smart Search\)**

  除了前面的两种搜索算法，还可以利用智能搜索算法，但是相对于前面的两种方法，智能搜索算法最大的缺点便是不能并行化。它的处理过程是一个序列，并只处理一部分候选点，然后对当前点进行评估，决定下一个点。智能搜索的目的是只对一部分点进行评估从而节省调优时间。   
  可以看出，智能搜索需要时间去计算下一个评估的点，于是相对于前面的方法，可能需要更多的时间。因此只有在对点进行评估所用的时间大于决定下一个需要评估的点的时间时才有意义。当然智能搜索算法也需要自己的超参数，因此也需要调优。有时好的智能搜索算法超参数可以确保智能搜索快于随机搜索。   
  文章前面提到，超参数调优是一个困难的过程，因为它不能想模型参数调优那样，给出一个形式化的数学函数，而对数学函数进行调优。因此很多优化算法，如牛顿优化算法、随机梯度下降算法都不能使用。目前有超参数三个智能调优算法：derivative-free optimization, Bayesian optimization和random forest smart tuning。derivative-free优化算法采用启发式来决定下一个计算的点；Bayesian和random forest优化算法都是创建一个响应函数曲面模型，由模型决定下一步需要计算的点。   
  [Jasper Snoek](http://bit.ly/snoek-adams)等使用高斯过程对响应函数进行建模。[Frank Hutter](http://bit.ly/hutter-hoos)等使用回归随机森林来近似这个响应曲面。 [Misha Bilenko](http://bit.ly/zheng-bilenko)等使用Nelder-Mead来进行超参数调优。



在本文的前部分已经讲述到，机器学习模型的评价分为离线评价与在线评价两个阶段。离线评价阶段发生在模型原型设计阶段，对不同的超参数、不同的特征集、不同模型进行评价，它是一个迭代的过程，使用选定的评价指标对每个迭代过程中生成的模型进行评价。一旦达到指定的迭代次数，则选择所有迭代中最优的模型，作为最终模型并投入到生产环境中使用。而在线评价则是使用一些商业评价指标来对模型进行评价以及更新。而A/B测试则属于在线测试。

**什么是A/B测试**

  A/B测试是目前在线测试中最主要的方法。该方法可以用来回答“新的模型比旧的模型更好吗（模型）？”、“这个按钮是使用黄色好一些还是蓝色好（设计）”等等问题。在A/B测试中，与两个部分：A和B，或控制/实验（control and experiment），A代表旧模型（设计）的评价，B代表新的模型（设计）的评价，然后将它们进行对比，得到是否新的模型比旧模型更好。当然是由具体的机制来进行衡量。   
  该衡量方法便是统计假设检验（statistical hypothesis testing）。其用来回答“新的模型或设计是否对一些关键的评价指标有着大幅度的提升或者明显的提升”。它包括两个对立的假设：空假设（null hypothesis）与替代假设（alternate hypothesis）。前者表示“新的模型或设计没有明显的提升”，后者则表示“新的模型或设计有了明显的提升”，“提升”具体反映在关键评价指标的平均值（mean value）等上面。   
  有很多书籍与网上资源对A/B测试有着详细的描述，这里不再累赘。如 [www.evanmiller.org](http://www.evanmiller.org/)，它对A/B测试进行了详细的讲解以及列举了一些工具。简而言之，A/B测试包括以下几个步骤：   
\* 随机划分成两组A与B   
\* 使用一些方法分别观察两组中的行为   
\* 计算一些统计指标   
\* 计算p-value   
\* 最后输出孰好孰坏   
  举个最简单的例子，在网页设计中使用A/B测试。首先需要建立一个测试页面（experiment page），这个页面可能在标题字体，背景颜色，措辞等方面与原有页面（control page）有所不同，然后将这两个页面以随机的方式同时推送给所有浏览用户。接下来分别统计两个页面的用户转化率，即可清晰的了解到两种设计的优劣。   
  A/B测试虽然通俗易懂，但是要想正确的使用它则较为棘手。下面将介绍一些在使用A/B测试时可能会遇到的一些陷阱与问题，这些问题包括理论层面的和实践层面的。

**A/B测试的一些陷阱**

**实验完全分离**

  在A/B测试中，需要将用户随机分为两组。一部分用户使用旧的模型或设计（如浏览原来的网页），另一部分用户使用新的模型或设计（如浏览新设计的网页）。那么需要保证experimentation组（使用新的模型或设计的组）的用户的纯净度，什么意思呢？   
  A/B测试中，划分为完全独立的，干净的两组是非常重要的。设想一下，在对网页中的按钮新样式进行测试时，需要确保统一用户[自始自终](https://www.baidu.com/s?wd=%E8%87%AA%E5%A7%8B%E8%87%AA%E7%BB%88&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)是使用同一个按钮设计，那么我们在对用户进行随机划分时，就需要使用能够唯一代表用户的来进行划分（即导流），如用户ID，用户sessions等。[Kohavi等的KDD 2012](http://bit.ly/deng-xu)论文表明一些使用旧设计的用户再使用新的设计时会带着某着偏见。

**使用什么评价指标**

  另一个重要的问题便是，在A/B测试中使用什么评价指标。因为A/B测试是在在线评价阶段，因此使用的评价指标便是商业指标。但是商业指标有没有离线阶段那些评价指标那么容易计算。举个例子，在搜索引擎中，一般对用户的数目、用户在结果站点的逗留时间、以及市场份额。在现实中，统计比较并不是那么容易，因此我们需要对独立用户每天访问数、平均会话时间长度等这些能够反映市场份额的指标进行计算，以便能够对市场份额进行估计。并且一般短期指标并不与长期指标保持一致。   
  在机器学习过程中，一般会用到四种类型的评价指标，分别是：训练评价指标（training metrics）、离线评价指标（验证评价指标，offline evaluation metrics or validation metrics）、新生数据评价指标（live metrics）、商业指标（business metrics）。训练评价指标是指模型优化的评价指标，即代价函数（目标函数或损失函数），如在线性回归中使用平方误差和、svm中分类平面几何间隔最大化等。离线评价指标是指模型训练完毕需要使用验证数据集来对模型进行评价，即前面所提到的那些指标，如分类模型评价指标、回归模型评价指标以及排序模型评价指标等。新生数据评价指标即使用模型上线后新生成的数据来评价模型，评价指标同离线评价指标，只是评价所用的数据不同。而商业指标即系统真正关心的最终指标，如转化率、点击率、PV访问量、UV访问量等。每个阶段使用的评价指标不一样，并且这些指标可能并不呈现线性相关，如在回归模型中，随着RMSE的下降，但是点击率（click-through rates.）并没有提高，详细可以参见[Kohavi‘s paper](http://bit.ly/deng-xu)。

**多少改变才算是真正的改变？**

  当确定了使用什么商业指标进行评价以及如何去计算这些指标时，接下来需要明确指标值提升了多少才算正在的提升，即多少的提升才可接受。这在某种程度上取决于实验的观察者数量。并且与问题2一样，它并不是一个数据科学范畴的问题，而是一个商业问题。因此需要根据经验挑选一个合适的值。

**单面测试还是双面测试（One-Sided or Two-Sided Test）？**

  单面测试只能告诉你新的模型是否比基准的是否更好，而无法告诉你是否更糟。因此需要进行双面测试，其不仅会告诉你新的模型是否会更好并且会告诉你是否更糟。是否更好与是否更糟需要进行分开对待。

**多少的FP\(False Positives\)能够忍受？**

  比基准模型更好，但是实际上确不是。FP的代价取决于实际应用。如在医药中，FP意味着病人使用无效药，这样便会对患者的健康造成很大的威胁。又如在机器学习中，FP意味着会使用一个认为会更有效的但却未更有效的模型来代替单前的模型。而FN意味着放弃了一个实际上会更有效的模型。   
  统计假设检验可以通过设定显著性水平（ the significance level）控制FP的概率，并通过测试的力（the power of the test.）度来控制FN的概率。   
  

**需要多少观察者？**

  观察者的数量由期望的统计功效（statistical power）部分决定。而统计功效在测试进行之前便需设定。一个常见的尝试时运行测试直到观察到一个重要的结果。这个方法是错误的。测试的力度是能够正确识别那些正样本。它能够使用显著性水平、A组的评价指标值与B组的评价指标值之差、观察者的个数这些去形式化地表示。选择合理的统计功效、显著水平等。然后选择每组中观察者的数量。[StitchFix](http://bit.ly/sf-power)与[Evan Miller’s website](http://bit.ly/miller-how-not)详细地进行了介绍。   
  

**评价指标是否满足高斯分布**

  A/B测试中绝大部分是使用T检验，但是T检验的所做出的假设前提并不是所有的评价指标都满足的。一个好的方法便是去查看指标的分布与检查T检验所做的假设是否有效。T检验假设是满足高斯分布的，那么评价指标释放满足高斯分布呢？通常，使用中心极限定理可以得到任何独立同分布并且具有期望与方差的随机变量都收敛于高斯分布。不难得出，评价指标假设以下条件成立：   
\* 指标值是采用平均值   
\* 指标值的分布是同一分布   
\* 指标值分布是对称的   
但是还是会有一些指标不满足的，如点击率是一个平均值，但是AUC却是一个积分值。并且可能不服从同一分布，如在A/B组中的用户种群不一样。同时，也有可能不是对称的。Kohavi等例举了一些例子说明评价指标不服从高斯分布，在这些例子中，标准误差不会随着测试的进行而降低。比如那些计数评价指标服从负二项式分布。当这些假设不成立时，分布遍不再收敛于高斯分布。 



### **P-value定义**

P-value（以下简称P值），又称“显著性水平”，它是指在原假设为真的条件下，样本数据拒绝原假设事件发生的概率，可以用来评估假设检验中最关键的第一类错误的概率。

今年3月，美国统计协会（ASA）在其官网上发布了《关于统计显著性和P值的声明》，进一步阐释了P值的概念和用处：

1）P值可以表达的是数据与一个给定模型（也就是原假设下的模型）不匹配的程度；

2）P值并不能衡量某条假设为真的概率，或是数据仅由随机因素产生的概率；

3）科学结论、商业决策或政策制定不应该仅依赖于P值是否超过一个给定的阈值；

4）合理的推断过程需要完整的报告和透明度；

5）P值或统计显著性并不衡量影响的大小或结果的重要性；

6）P值就其本身而言，并不是一个非常好的对模型或假设所含证据大小的衡量。

### **P-value的计算——T检验**

P值的计算公式取决于假设检验的具体方式，常用的假设检验方法有Z检验、T检验和卡方检验等，不同的方法有不同的适用条件和检验目标。

A/B测试中是用对照版本和试验版本两个样本的数据来对这两个总体是否存在差异进行检验，所以适合使用T检验方法中的独立双样本检验 \(independent two-samples t test\)。通过T分布理论来计算相关的概率水平，也就是P-value的值。

T检验的计算公式，首先通过来公式计算出统计检验量Z值，公式中的相关组成因素就是：两个版本的各自均值、方差（标准差），以及样本的大小，从而推算出统计量的Z值是多少。

![](../../.gitbook/assets/image%20%2851%29.png)

然后通过t分布（大样本情况下近似正态分布）的公式计算得出和Z值对应的P值，阴影部分的面积就是P-value的值。

![](../../.gitbook/assets/image%20%2850%29.png)

P值算出来之后，我们就可以根据P值按照前面介绍的假设检验决策规则来判断这两个样本均值的差异是否显著了。








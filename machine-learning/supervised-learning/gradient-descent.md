# Gradient Descent

**梯度：**

在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数 $$f(x,y)$$, 分别对x,y求偏导数，求得的梯度向量就是 $$(\frac{∂f}{∂x}, \frac{∂f}{∂y})^T$$, 简称 grad$$f(x,y)$$ 或者 $$▽f(x,y)$$ 。对于在点 $$ (x_0,y_0)$$ 的具体梯度向量就是 $$(\frac{∂f}{∂x_0}, \frac{∂f}{∂y_0})^T$$ .或者 $$▽f(x_0,y_0)$$，如果是3个参数的向量梯度，就是 $$(\frac{∂f}{∂x}, \frac{∂f}{∂y}, \frac{∂f}{∂z})^T$$ ,以此类推。

那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数 $$ f(x,y)$$,在点 $$(x_0,y_0)$$，沿着梯度向量的方向就是 $$(\frac{∂f}{∂x_0}, \frac{∂f}{∂y_0})^T$$ 的方向是 $$f(x,y)$$ 增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是 $$-(\frac{∂f}{∂x_0}, \frac{∂f}{∂y_0})^T$$ 的方向，梯度减少最快，也就是更加容易找到函数的最小值。

梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。

**梯度下降的几个概念：**

1. 步长/学习率（Learning rate）：步长决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。

2.特征（feature）：指的是样本中输入部分，比如2个单特征的样本 $$（x^{(0)},y^{(0)}）,（x^{(1)},y^{(1)}）$$ ,则第一个样本特征为 $$x^{(0)}$$ ，第一个样本输出为 $$y^{(0)}$$ 。

3. 假设函数（hypothesis function）：在监督学习中，为了拟合输入样本，而使用的假设函数，记为 $$h_{\theta}(x)$$ 。比如对于单个特征的m个样本 $$（x^{(i)},y^{(i)}）(i=1,2,...m)$$ 可以采用拟合函数如下： $$h_{\theta}(x) = \theta_0+\theta_1x$$ 。

4. 损失函数（loss function）：为了评估模型拟合的好坏，通常用损失函数来度量拟合的程度。损失函数极小化，意味着拟合程度最好，对应的模型参数即为最优参数。在线性回归中，损失函数通常为样本输出和假设函数的差取平方。比如对于m个样本 $$（x_i,y_i）(i=1,2,...m)$$ ,采用线性回归，损失函数为：

             $$J(\theta_0, \theta_1) = \sum\limits_{i=1}^{m}(h_\theta(x_i) - y_i)^2$$ 

其中 $$x_{i}$$ 表示第i个样本特征， $$y_{i}$$ 表示第i个样本对应的输出， $$h_\theta(x_i)$$ 为假设函数。   

**算法过程：**

1）确定当前位置的损失函数的梯度，对于 $$θ $$ 向量,其梯度表达式如下：

　　　　　　　　 $$\frac{\partial}{\partial\mathbf\theta}J(\mathbf\theta)$$ 

2）用步长乘以损失函数的梯度，得到当前位置下降的距离.

3）确定 $$θ$$ 向量里面的每个值,梯度下降的距离都小于 $$ε$$ ，如果小于 $$ε$$ 则算法终止，当前 $$θ$$ 向量即为最终结果。否则进入步骤4.

4）更新 $$θ$$ 向量，其更新表达式如下。更新完毕后继续转入步骤1.

　　　　　　　　 $$\mathbf\theta= \mathbf\theta - \alpha\frac{\partial}{\partial\theta}J(\mathbf\theta)$$   
**算法调优：**

1. 算法的步长选择
2. 算法参数的初始值选择
3. 归一化

Batch Gradient Descent

Mini-batch Gradient Descent

Stochastic Gradient Descent

Newton

Adam


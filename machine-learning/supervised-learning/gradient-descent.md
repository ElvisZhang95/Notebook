# Gradient Descent

**梯度：**

在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数 $$f(x,y)$$, 分别对x,y求偏导数，求得的梯度向量就是 $$(\frac{∂f}{∂x}, \frac{∂f}{∂y})^T$$, 简称 grad$$f(x,y)$$ 或者 $$▽f(x,y)$$ 。对于在点 $$ (x_0,y_0)$$ 的具体梯度向量就是 $$(\frac{∂f}{∂x_0}, \frac{∂f}{∂y_0})^T$$ .或者 $$▽f(x_0,y_0)$$，如果是3个参数的向量梯度，就是 $$(\frac{∂f}{∂x}, \frac{∂f}{∂y}, \frac{∂f}{∂z})^T$$ ,以此类推。

那么这个梯度向量求出来有什么意义呢？他的意义从几何意义上讲，就是函数变化增加最快的地方。具体来说，对于函数 $$ f(x,y)$$,在点 $$(x_0,y_0)$$，沿着梯度向量的方向就是 $$(\frac{∂f}{∂x_0}, \frac{∂f}{∂y_0})^T$$ 的方向是 $$f(x,y)$$ 增加最快的地方。或者说，沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，也就是 $$-(\frac{∂f}{∂x_0}, \frac{∂f}{∂y_0})^T$$ 的方向，梯度减少最快，也就是更加容易找到函数的最小值。

梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。

Batch Gradient Descent

Mini-batch Gradient Descent

Stochastic Gradient Descent

Newton

Adam


# Decision Tree

什么是决策树（Decision Tree）：

决策树是一种分类和回归的基本模型，可从三个角度来理解它，即：

* 一棵树
* if-then规则的集合，该集合是决策树上的所有从根节点到叶节点的路径的集合
* 定义在特征空间与类空间上的条件概率分布，决策树实际上是将特征空间划分成了互不相交的单元，每个从根到叶的路径对应着一个单元。决策树所表示的条件概率分布由各个单元给定条件下类的条件概率分布组成。实际中，哪个类别有较高的条件概率，就把该单元中的实例强行划分为该类别。

主要的优点有两个：

* 模型具有可解释性，容易向业务部门人员描述。
* 分类速度快

当然也有其他优点，比如可以同时处理类别数据和数值数据。在运用分类决策树分类时，我们的数据一般会进行离散化

学习过程：

决策树的学习本质上就是从训练数据集中归纳出一组分类规则，使它与训练数据矛盾较小的同时具有较强的泛华能力。从另一个角度看，学习也是基于训练数据集估计条件概率模型（至此，回答完了模型部分，下面接着说策略和算法）。  
 决策树的损失函数通常是**正则化的极大似然函数**，学习的策略是以损失函数为目标函数的最小化（说完了策略，该说算法了）。

由于这个最小化问题是一个NP完全问题，现实中，我们通常采用启发式算法\(Min-Max\)（这里，面试官可能会问什么是启发式算法，要有准备，SMO算法就是启发式算法）来近似求解这一最优化问题，得到的决策树是次最优的。

该启发式算法可分为三步：

* 特征选择
* 模型生成
* 决策树的剪枝




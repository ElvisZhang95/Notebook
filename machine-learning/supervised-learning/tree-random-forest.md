# Tree/ Random Forest

在AdaBoost算法中所有被分错的样本的权重更新比例相同 AdaBoost算法中不同的训练集是通过调整每个样本对应的权重来实现的。开始时，每个样本对应的权重是相同的，即其中n为样本个数，在此样本分布下训练出一弱分类器。对于分类错误的样本，加大其对应的权重；而对于分类正确的样本，降低其权重，这样分错的样本就被凸显出来，从而得到一个新的样本分布。在新的样本分布下，再次对样本进行训练，得到弱分类器。以此类推，将所有的弱分类器重叠加起来，得到强分类器。

 Boost和Bagging都是组合多个分类器投票的方法，二者均是根据单个分类器的正确率决定其权重。 Bagging与Boosting的区别：取样方式不同。Bagging采用均匀取样，而Boosting根据错误率取样。Bagging的各个预测函数没有权重，而Boosting是由权重的，Bagging的各个预测函数可以并行生成，而Boosing的各个预测函数只能顺序生成。


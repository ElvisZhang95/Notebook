# Linear/ Logistic Regression

**Linear Regression:**

**y**是一列有n个观测值的观测变量，或者直接说因变量以便于理解，

**X**是由多列特征组成的特征空间，假设有p列特征，简单理解就是有p个自变量，每个特征都有n个值，这与y是对应的：

![](https://www.zhihu.com/equation?tex=y%3D%5Cleft%28+%5Cbegin%7Barray%7D%7Bccc%7D+y_1+%5C%5C+y_2+%5C%5C+%5Cvdots+%5C%5C+y_n%5C%5C+%5Cend%7Barray%7D+%5Cright%29+%5Cquad%2C%5Cquad+X%3D+%5Cleft%28+%5Cbegin%7Barray%7D%7Bccc%7D+X_1%5ET+%5C%5C+X_2%5ET+%5C%5C+%5Cvdots%5C%5C+X_n%5ET+%5C%5C+%5Cend%7Barray%7D+%5Cright%29+%3D+%5Cleft%28+%5Cbegin%7Barray%7D%7Bccc%7D+1+%26+x_%7B11%7D+%26+%5Ccdots+%26+x_%7B1p%7D%5C%5C+1+%26+x_%7B21%7D+%26+%5Ccdots+%26+x_%7B2p%7D%5C%5C+%5Cvdots+%26+%5Cvdots+%26+%5Cddots+%26+%5Cvdots%5C%5C+1+%26+x_%7Bn1%7D+%26+%5Ccdots+%26+x_%7Bnp%7D%5C%5C+%5Cend%7Barray%7D+%5Cright%29%5C%5C)

**β**是系数向量，**ε**是干扰项（disturbance term），或称错误项（error term）：

![](https://www.zhihu.com/equation?tex=%CE%B2%3D%5Cleft%28+%5Cbegin%7Barray%7D%7Bccc%7D+%CE%B2_0+%5C%5C+%CE%B2_1%5C%5C%CE%B2_2+%5C%5C+%5Cvdots+%5C%5C+%CE%B2_p%5C%5C+%5Cend%7Barray%7D+%5Cright%29%5Cquad%2C%5Cquad+%CE%B5%3D%5Cleft%28+%5Cbegin%7Barray%7D%7Bccc%7D+%CE%B5_1+%5C%5C+%CE%B5_2+%5C%5C+%5Cvdots+%5C%5C+%CE%B5_n%5C%5C+%5Cend%7Barray%7D+%5Cright%29%5C%5C)

最后我们得到的第i个y（观测值）是这样的：

![](https://www.zhihu.com/equation?tex=y_i%3D%CE%B2_01%2B%CE%B2_1x_%7Bi1%7D%2B%5Ccdots%2B%CE%B2_px_%7Bip%7D%2B%CE%B5_i+%28i%3D1%2C%5Ccdots%2Cn%2C%CE%B2_0%E4%B8%BA%E6%88%AA%E8%B7%9D%29%5C%5C)

**所以，线性回归的公式是这样子的：**

![](https://www.zhihu.com/equation?tex=y%3DX%CE%B2%2B%CE%B5+%5C%5C)

前面说过，线性回归的过程就是要**找到最优的模型来描述数据**。这里就产生了两个问题：

* 如何定义“最优”？
* 如何寻找“最优”？

想要评价一个模型的优良，就需要一个**度量标准**。对于回归问题，最常用的度量标准就是[**均方差（MSE，Mean Squared Error）**](http://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Mean_squared_error)，均方差是指预测值和实际值之间的平均方差。平均方差越小，说明测试值和实际值之间的差距越小，即模型性能更优。

在线性回归的式子中y和X是给定的，而β和ε是不确定的，也就是说，**找到最优的β和ε，就找到了最优的模型**。

综合以上结论，可以用如下式子描述：

![](https://www.zhihu.com/equation?tex=%28%CE%B2%5E%2A%2C%CE%B5%5E%2A%29%3Dargmin+%5Csum_%7Bi%3D1%7D%5Em%28f%28x_i%29-y_i%29%5E2+%5C%5C)

其中， ![&#x3B2;^\*](https://www.zhihu.com/equation?tex=%CE%B2%5E%2A) 和 ![&#x3B5;^\*](https://www.zhihu.com/equation?tex=%CE%B5%5E%2A) 是要求的最优参数，右部是最小化均方差。

明确了我们的目标，接下来就是该如何去寻找这两个量呢？最常用的是参数估计方法是[**最小二乘法（Least Square Method）**](http://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%25E6%259C%2580%25E5%25B0%258F%25E4%25BA%258C%25E4%25B9%2598%25E6%25B3%2595), 最小二乘法试图找到一条直线，使得样本点和直线的[欧氏距离](http://link.zhihu.com/?target=https%3A//zh.wikipedia.org/zh-hans/%25E6%25AC%25A7%25E5%2587%25A0%25E9%2587%258C%25E5%25BE%2597%25E8%25B7%259D%25E7%25A6%25BB)之和最小。这个寻找的过程简单描述就是：根据凸函数的性质，求其关于β和ε的二阶导的零点。

对于large scale data，我们也可以采用Gradient Descent的方法来进行优化。

_**A simple linear system that can be solved in**_ $$O((dim \theta)^3)$$ ****_**time.**_



**Logistic Regression:**

一句话概括： **逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。**

这里面其实包含了5个点 1：逻辑回归的假设，2：逻辑回归的损失函数，3：逻辑回归的求解方法，4：逻辑回归的目的，5:逻辑回归如何分类。

* **逻辑回归的基本假设**
  * 任何的模型都是有自己的假设，在这个假设下模型才是适用的。逻辑回归的**第一个**基本假设是**假设数据服从伯努利分布。**伯努利分布有一个简单的例子是抛硬币，抛中为正面的概率是 $$p $$ ,抛中为负面的概率是 $$1-p $$ .在逻辑回归这个模型里面是假设 $$h_θ(x) $$ 为样本为正的概率， $$1−h_θ(x)$$ 为样本为负的概率。那么整个模型可以描述为 $$h_\theta\left(x;\theta \right )=p$$ 
  * 逻辑回归的第二个假设是假设样本为正的概率是 $$p=\frac{1}{1+e^{-\theta^{T} x}}$$ 
  * 所以逻辑回归的最终形式 $$h_\theta\left(x;\theta \right )=\frac{1}{1+e^{-\theta^{T} x}}$$ 
* **逻辑回归的损失函数**
  * 逻辑回归的损失函数是它的极大似然函数
  * $$L_\theta\left(x\right )= \prod _{i=1}^{m}h_\theta(x^{i};\theta )^{y{i}}*(1-h_\theta(x^{i};\theta))^{1-y^{i}}$$ 
* **逻辑回归的求解方法**
  * 由于该极大似然函数无法直接求解，我们一般通过对该函数进行梯度下降来不断逼急最优解。在这个地方其实会有个加分的项，考察你对其他优化方法的了解。因为就梯度下降本身来看的话就有随机梯度下降，批梯度下降，small batch 梯度下降三种方式，面试官可能会问这三种方式的优劣以及如何选择最合适的梯度下降方式。
    * 简单来说 批梯度下降会获得全局最优解，缺点是在更新每个参数的时候需要遍历所有的数据，计算量会很大，并且会有很多的冗余计算，导致的结果是当数据量大的时候，每个参数的更新都会很慢。
    * 随机梯度下降是以高方差频繁更新，优点是使得sgd会跳到新的和潜在更好的局部最优解，缺点是使得收敛到局部最优解的过程更加的复杂。
    * 小批量梯度下降结合了sgd和batch gd的优点，每次更新的时候使用n个样本。减少了参数更新的次数，可以达到更加稳定收敛结果，一般在深度学习当中我们采用这种方法。
* * 其实这里还有一个隐藏的更加深的加分项，看你了不了解[诸如Adam，动量法等优化方法](gradient-descent.md)。因为上述方法其实还有两个致命的问题。
    * 第一个是如何对模型选择合适的学习率。自始至终保持同样的学习率其实不太合适。因为一开始参数刚刚开始学习的时候，此时的参数和最优解隔的比较远，需要保持一个较大的学习率尽快逼近最优解。但是学习到后面的时候，参数和最优解已经隔的比较近了，你还保持最初的学习率，容易越过最优点，在最优点附近来回振荡，通俗一点说，就很容易学过头了，跑偏了。
    * 第二个是如何对参数选择合适的学习率。在实践中，对每个参数都保持的同样的学习率也是很不合理的。有些参数更新频繁，那么学习率可以适当小一点。有些参数更新缓慢，那么学习率就应该大一点。这里我们不展开，有空我会专门出一个专题介绍。
* **逻辑回归的目的**
  * 该函数的目的便是将数据二分类，提高准确率。
* **逻辑回归如何分类**
  * 逻辑回归作为一个回归\(也就是y值是连续的\)，如何应用到分类上去呢。y值确实是一个连续的变量。逻辑回归的做法是划定一个阈值，y值大于这个阈值的是一类，y值小于这个阈值的是另外一类。阈值具体如何调整根据实际情况选择。一般会选择0.5做为阈值来划分。

**3.对逻辑回归的进一步提问**

    逻辑回归虽然从形式上非常的简单，但是其内涵是非常的丰富。有很多问题是可以进行思考的

* **逻辑回归的损失函数为什么要使用极大似然函数作为损失函数？**
  * 损失函数一般有四种，平方损失函数，对数损失函数，HingeLoss0-1损失函数，绝对值损失函数。将极大似然函数取对数以后等同于对数损失函数。在逻辑回归这个模型下，对数损失函数的训练求解参数的速度是比较快的。至于原因大家可以求出这个式子的梯度更新 $$\theta _j=\theta _j-\left ( y^{i} -h_\theta (x^{i};\theta ) \right )\ast x^{i}_j$$ 这个式子的更新速度只和 $$x_{j}^i$$ ， $$y^i$$ 相关。和sigmod函数本身的梯度是无关的。这样更新的速度是可以自始至终都比较的稳定。
  * 为什么不选平方损失函数的呢？其一是因为如果你使用平方损失函数，你会发现梯度更新的速度和sigmod函数本身的梯度是很相关的。sigmod函数在它在定义域内的梯度都不大于0.25。这样训练会非常的慢。
* **逻辑回归在训练的过程当中，如果有很多的特征高度相关或者说有一个特征重复了100遍，会造成怎样的影响？**
* 先说结论，如果在损失函数最终收敛的情况下，其实就算有很多特征高度相关也不会影响分类器的效果。
* 但是对特征本身来说的话，假设只有一个特征，在不考虑采样的情况下，你现在将它重复100遍。训练以后完以后，数据还是这么多，但是这个特征本身重复了100遍，实质上将原来的特征分成了100份，每一个特征都是原来特征权重值的百分之一。
* 如果在随机采样的情况下，其实训练收敛完以后，还是可以认为这100个特征和原来那一个特征扮演的效果一样，只是可能中间很多特征的值正负相消了。
* **为什么我们还是会在训练的过程当中将高度相关的特征去掉？**
  * 去掉高度相关的特征会让模型的可解释性更好
  * 可以大大提高训练的速度。如果模型当中有很多特征高度相关的话，就算损失函数本身收敛了，但实际上参数是没有收敛的，这样会拉低训练的速度。其次是特征多了，本身就会增大训练的时间。

**4.逻辑回归的优缺点总结**

    面试的时候，别人也经常会问到，你在使用逻辑回归的时候有哪些感受。觉得它有哪些优缺点。

     **在这里我们总结了逻辑回归应用到工业界当中一些优点：**

* 形式简单，模型的可解释性非常好。从特征的权重可以看到不同的特征对最后结果的影响，某个特征的权重值比较高，那么这个特征最后对结果的影响会比较大。
* 模型效果不错。在工程上是可以接受的（作为baseline\)，如果特征工程做的好，效果不会太差，并且特征工程可以大家并行开发，大大加快开发的速度。
* 训练速度较快。分类的时候，计算量仅仅只和特征的数目相关。并且逻辑回归的分布式优化sgd发展比较成熟，训练的速度可以通过堆机器进一步提高，这样我们可以在短时间内迭代好几个版本的模型。
* 资源占用小,尤其是内存。因为只需要存储各个维度的特征值，。
* 方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值\(大于某个阈值的是一类，小于某个阈值的是一类\)。

      **但是逻辑回归本身也有许多的缺点:**

* 准确率并不是很高。因为形式非常的简单\(非常类似线性模型\)，很难去拟合数据的真实分布。
* 很难处理数据不平衡的问题。举个例子：如果我们对于一个正负样本非常不平衡的问题比如正负样本比 10000:1.我们把所有样本都预测为正也能使损失函数的值比较小。但是作为一个分类器，它对正负样本的区分能力不会很好。
* 处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据，或者进一步说，处理二分类的问题 。
* 逻辑回归本身无法筛选特征。有时候，我们会用gbdt来筛选特征，然后再上逻辑回归。





面试题：

1. Linear regression与Logistic Regression不同：

```text
1）线性回归要求变量服从正态分布，logistic回归对变量分布没有要求。

2）线性回归要求因变量（y）是连续性数值变量，而logistic回归要求因变量是分类型变量。

3）线性回归要求自变量(x)和因变量呈线性关系，而logistic回归不要求自变量和因变量呈线性关系

4）logistic回归是分析因变量取某个值的概率与自变量的关系，而线性回归是直接分析因变量与自变量的关系

总之, logistic回归与线性回归实际上有很多相同之处，最大的区别就在于他们的因变量不同，其他的基本都差不多，
正是因为如此，这两种回归可以归于同一个家族，即广义线性模型（generalized linear model）。
这一家族中的模型形式基本上都差不多，不同的就是因变量不同，如果是连续的，就是多重线性回归，
如果是二项分布，就是logistic回归。logistic回归的因变量可以是二分类的，也可以是多分类的，
但是二分类的更为常用，也更加容易解释。所以实际中最为常用的就是二分类的logistic回归。
```

2. 线性回归的前提：

```text
线性回归需要满足四个前提假设
1. Linearity 线性
应变量和每个自变量都是线性关系。
2. Indpendence 独立性
对于所有的观测值，它们的误差项相互之间是独立的。
3. Normality 正态性
误差项服从正态分布。
4. Equal-variance 等方差
所有的误差项具有同样方差。
这四个假设的首字母，合起来就是LINE，这样很好记。

其实除了LINE条件外，多元线性回归还需要满足一个条件：自变量X之间不存在完全多重共线性。

线性回归使用普通最小二乘法（OLS）进行参数估计，上述LINE假定条件也称为OLS假定，用数学语言来说OLS假定就是：

OLS假定1：数据矩阵X列满秩（不存在完全多重共线性）；
OLS假定2：随机误差项ε的总体均值为0，即 E[ε]=0 ​；换句话说就是除了解释变量X之外，放在随机干扰项中的其他因素对于因变量Y的影响总体上有正有负，但均值为0（正态性）；
OLS假定3：随机误差项ε与自变量X不相关；
OLS假定4：随机误差项ε同方差且互不相关，即​ E[εε^T]=σ^2I （无自相关、等方差性）；
OLS假定5：随机误差项满足正态分布（正态性）。

由于OLS估计量 \hat\beta 本身就是一个随机变量，需要对其估计值的优劣性进行判断。
判断​的准则有无偏性、有效性和一致性等准则。上面的假定条件正是由这些准则导出的。
```


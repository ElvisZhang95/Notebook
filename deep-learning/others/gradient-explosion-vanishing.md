# Gradient Explosion/Vanishing

梯度消失：这本质上是由于激活函数的选择导致的， 最简单的sigmoid函数为例，在函数的两端梯度求导结果非常小（饱和区），导致后向传播过程中由于多次用到激活函数的导数值使得整体的乘积梯度结果变得越来越小，也就出现了梯度消失的现象。

梯度爆炸：同理，出现在激活函数处在激活区，而且权重W过大的情况下。但是梯度爆炸不如梯度消失出现的机会多。

**解决方法：**

* 预训练加微调
* 梯度剪切、权重正则（针对梯度爆炸）
* 使用不同的激活函数
* 使用Batch Normalisation \(BN\) 
* 使用残差结构
* 使用LSTM网络



_**预训练加微调**：**pre-training&fine-tunning**_ 

`该方法由Hinton在2006年提出，采取先局部后整体的思想，pre-training先逐层训练每一个隐层的神经元，fine-tunning再通过BP对整个网络进行训练微调。`

 _**梯度剪切、正则 （主要用在防止梯度爆炸上）**_

梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。

![](../../.gitbook/assets/image%20%2844%29.png)

`注：在WGAN中也有梯度剪切限制操作，但是和这个是不一样的，WGAN限制梯度更新信息是为了保证lipchitz条件。`

另外一种解决梯度爆炸的手段是采用 _**权重正则化（Weights regularisation）**_

比较常见的是 $$L1$$ 正则，和 $$L2$$ 正则，在各个深度框架中都有相应的API可以使用正则化，比如在 $$tensorflow$$ 中，若搭建网络的时候已经设置了正则化参数，则调用以下代码可以直接计算出正则损失

```text
regularization_loss = tf.add_n(tf.losses.get_regularization_losses(scope='my_resnet_50'))
```

如果没有设置初始化参数，也可以使用以下代码计算 $$L2$$ 正则损失：

```text
l2_loss = tf.add_n([tf.nn.l2_loss(var) for var in tf.trainable_variables() if 'weights' in var.name])
```

正则化是通过对网络权重做正则限制过拟合，仔细看正则项在损失函数的形式：

$$
Loss=(y−W^Tx)^2+α∣∣W∣∣ ^2
$$

其中， $$\alpha$$ 是指正则项系数，因此，如果发生梯度爆炸，权值的范数就会变的非常大，通过正则化项，可以部分限制梯度爆炸的发生。

```text
注：事实上，在深度神经网络中，往往是梯度消失出现的更多一些。
```



_**使用不同的激活函数**_

**Relu:** _****_思想也很简单，如果激活函数的导数为1，那么就不存在梯度消失爆炸的问题了，每层的网络都可以得到相同的更新速度，relu就这样应运而生。先看一下relu的数学表达式：

![](../../.gitbook/assets/image%20%288%29.png)



![](../../.gitbook/assets/image%20%2855%29.png)

从上图中，我们可以很容易看出，relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。

| 优点 | 缺点 |
| :--- | :--- |
| 解决了梯度消失、爆炸的问题 | 由于负数部分恒为0，会导致一些神经元无法激活（可通过设置小学习率部分解决） |
| 计算方便，计算速度快，加快网络训练 | 输出不是以0为中心的 |

还有像其他的**leakReLu, elu**等激活函数为了解决ReLu的0区间的部分做了修改。

$$leakReLu=max(k∗x,x)  $$ 

`其中k是leak系数，一般选择0.01或者0.02，或者通过学习而来。leakReLu解决了0区间带来的影响，而且包含了relu的所有优点`

**elu**

![](../../.gitbook/assets/image%20%283%29.png)

![](../../.gitbook/assets/image%20%2831%29.png)

比起leakReLu来 计算时间会更耗时一些。



_**Batch Normalization \(BN\)**_

即批规范化，通过规范化操作将输出信号x规范化保证网络的稳定性。具有加速网络收敛速度，提升训练稳定性的效果。

正向传播中 $$f_{2} = f_{1} (w^{T} * x +b)$$，那么反向传播中， $$\frac{\partial f_{2} }{\partial x} = \frac{\partial f_{2}}{\partial f_{1}} w$$ , 反向传播式子中有 $$w$$ 的存在，所以 $$w$$ 的大小影响了梯度的消失和爆炸。Batch Normalisation就是通过对每一层的输出规范为均值和方差一致的方法，消除了 $$w$$ 带来的方法缩小的影响，进而解决梯度消失和爆炸的问题。

具体实现原理： [https://blog.csdn.net/qq\_25737169/article/details/79048516](https://blog.csdn.net/qq_25737169/article/details/79048516)

​**一句话：每一层的输出进行规范化（减均值除方差）以后进入激活函数进入下一层。**

\*\*\*\*

_**残差结构**_

传统的BP网络都是残差逐层串行传递，而该网络支持残差跨层传递，直接避免了连乘带来的计算不稳定性。

_\*\*\*\*_

_**LSTM**_

LSTM，长短期记忆网络（long-short term memory networks），不那么容易发生梯度消失的，主要原因在于LSTM内部复杂的门。



_\*\*\*\*_

_\*\*\*\*_

_\*\*\*\*_

_\*\*\*\*_

_\*\*\*\*_

_\*\*\*\*_

_\*\*\*\*_

_\*\*\*\*_

_\*\*\*\*_

_\*\*\*\*_




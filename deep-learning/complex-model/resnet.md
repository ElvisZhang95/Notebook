# ResNet

 我们设深度网络中某隐含层为H（x）-x→F（x），如果可以假设多个非线性层组合可以近似于一个复杂函数，那么也同样可以假设隐含层的残差近似于某个复杂函数\[6\]。即那么我们可以将隐含层表示为H（x）=F（x）+ x。  
  
这样一来，我们就可以得到一种全新的残差结构单元，如图3所示。可以看到，残差单元的输出由多个卷积层级联的输出和输入元素间相加\(保证卷积层输出和输入元素维度相同\)，再经过ReLU激活后得到。将这种结构级联起来，就得到了残差网络。典型网络结构表1所示。  
![](https://pic2.zhimg.com/80/90e58f36fc1b0ae42443b69176cc2a75_hd.png)

在上图的残差网络结构图中，通过“shortcut connections（捷径连接）”的方式，直接把输入x传到输出作为初始结果，输出结果为H\(x\)=F\(x\)+x，当F\(x\)=0时，那么H\(x\)=x，也就是上面所提到的恒等映射。于是，ResNet相当于将学习目标改变了，不再是学习一个完整的输出，而是目标值H\(X\)和x的差值，也就是所谓的残差F\(x\) := H\(x\)-x，因此，后面的训练目标就是要将残差结果逼近于0，使到随着网络加深，准确率不下降。

这种残差跳跃式的结构，打破了传统的神经网络n-1层的输出只能给n层作为输入的惯例，使某一层的输出可以直接跨过几层作为后面某一层的输入，其意义在于为叠加多层网络而使得整个学习模型的错误率不降反升的难题提供了新的方向。 至此，神经网络的层数可以超越之前的约束，达到几十层、上百层甚至千层，为高级语义特征提取和分类提供了可行性。

![](../../.gitbook/assets/image%20%2812%29.png)

因为经过“shortcut connections（捷径连接）”后，H\(x\)=F\(x\)+x，如果F\(x\)和x的通道相同，则可直接相加，那么通道不同怎么相加呢。上图中的实线、虚线就是为了区分这两种情况的：

* 实线的Connection部分，表示通道相同，如上图的第一个粉色矩形和第三个粉色矩形，都是3x3x64的特征图，由于通道相同，所以采用计算方式为H\(x\)=F\(x\)+x
* 虚线的的Connection部分，表示通道不同，如上图的第一个绿色矩形和第三个绿色矩形，分别是3x3x64和3x3x128的特征图，通道不同，采用的计算方式为H\(x\)=F\(x\)+Wx，其中W是卷积操作，用来调整x维度的。

除了上面提到的两层残差学习单元，还有三层的残差学习单元，如下图所示：

![](../../.gitbook/assets/image%20%2814%29.png)

两种结构分别针对ResNet34（左图）和ResNet50/101/152（右图），其目的主要就是为了降低参数的数目。左图是两个3x3x256的卷积，参数数目: 3x3x256x256x2 = 1179648，右图是第一个1x1的卷积把256维通道降到64维，然后在最后通过1x1卷积恢复，整体上用的参数数目：1x1x256x64 + 3x3x64x64 + 1x1x64x256 = 69632，右图的参数数量比左图减少了16.94倍，因此，右图的主要目的就是为了减少参数量，从而减少计算量。 对于常规的ResNet，可以用于34层或者更少的网络中（左图）；对于更深的网络（如101层），则使用右图，_**其目的是减少计算和参数量。**_



可以注意到残差网络有这样几个特点：

* 网络较瘦，控制了参数数量；
* 存在明显层级，特征图个数逐层递进，保证输出特征表达能力
* 使用了较少的池化层，大量使用下采样，提高传播效率
* 4. 没有使用Dropout，利用BN和全局平均池化进行正则化，加快了训练速度
* 层数较高时减少了3x3卷积个数，并用1x1卷积控制了3x3卷积的输入输出特征图数量，称这种结构为“瓶颈”\(bottleneck\)。


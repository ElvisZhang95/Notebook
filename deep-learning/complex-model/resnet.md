# ResNet

概括：大致就是随着深度的加深可能产生的vanishing/exploding gradients由normalized initialization和intermediate normalization解决了（读到的时候心理那叫一个刺激...）。如作者所说，他们确认了反向传播中的梯度，认为BN已经确保避免了梯度消弭情况的发生，因此，退化问题不太可能是由于梯度消散引起的。在排除梯度原因之后，作者猜测深层网络的退化问题可能是由于普通深层网络呈现指数级低收敛速度导致，至于为什么会这样，还有待研究。

残差网络主要解决了网络退化的问题（随着网络的深度增加，准确度反而下降了）。在于恒等映射给出了一个合理的先验，使得残差函数更容易被拟合。还有特征融合的想法（即融合了高维特征与低维特征），也挺meaningful。

即增加一个identity mapping（恒等映射），将原始所需要学的函数H\(x\)转换成F\(x\)+x，而作者认为这两种表达的效果相同，但是优化的难度却并不相同，作者假设F\(x\)的优化 会比H\(x\)简单的多。这一想法也是源于图像处理中的残差向量编码，通过一个reformulation，将一个问题分解成多个尺度直接的残差问题，能够很好的起到优化训练的效果。 这个Residual block通过shortcut connection实现，通过shortcut将这个block的输入和输出进行一个element-wise的加叠，这个简单的加法并不会给网络增加额外的参数和计算量，同时却可以大大增加模型的训练速度、提高训练效果，并且当模型的层数加深时，这个简单的结构能够很好的解决退化问题。



 我们设深度网络中某隐含层为H（x）-x→F（x），如果可以假设多个非线性层组合可以近似于一个复杂函数，那么也同样可以假设隐含层的残差近似于某个复杂函数\[6\]。即那么我们可以将隐含层表示为H（x）=F（x）+ x。  
  
这样一来，我们就可以得到一种全新的残差结构单元，如图3所示。可以看到，残差单元的输出由多个卷积层级联的输出和输入元素间相加\(保证卷积层输出和输入元素维度相同\)，再经过ReLU激活后得到。将这种结构级联起来，就得到了残差网络。典型网络结构表1所示。  
![](https://pic2.zhimg.com/80/90e58f36fc1b0ae42443b69176cc2a75_hd.png)

在上图的残差网络结构图中，通过“shortcut connections（捷径连接）”的方式，直接把输入x传到输出作为初始结果，输出结果为H\(x\)=F\(x\)+x，当F\(x\)=0时，那么H\(x\)=x，也就是上面所提到的恒等映射。于是，ResNet相当于将学习目标改变了，不再是学习一个完整的输出，而是目标值H\(X\)和x的差值，也就是所谓的残差F\(x\) := H\(x\)-x，因此，后面的训练目标就是要将残差结果逼近于0，使到随着网络加深，准确率不下降。

这种残差跳跃式的结构，打破了传统的神经网络n-1层的输出只能给n层作为输入的惯例，使某一层的输出可以直接跨过几层作为后面某一层的输入，其意义在于为叠加多层网络而使得整个学习模型的错误率不降反升的难题提供了新的方向。 至此，神经网络的层数可以超越之前的约束，达到几十层、上百层甚至千层，为高级语义特征提取和分类提供了可行性。

![](../../.gitbook/assets/image%20%2815%29.png)

因为经过“shortcut connections（捷径连接）”后，H\(x\)=F\(x\)+x，如果F\(x\)和x的通道相同，则可直接相加，那么通道不同怎么相加呢。上图中的实线、虚线就是为了区分这两种情况的：

* 实线的Connection部分，表示通道相同，如上图的第一个粉色矩形和第三个粉色矩形，都是3x3x64的特征图，由于通道相同，所以采用计算方式为H\(x\)=F\(x\)+x
* 虚线的的Connection部分，表示通道不同，如上图的第一个绿色矩形和第三个绿色矩形，分别是3x3x64和3x3x128的特征图，通道不同，采用的计算方式为H\(x\)=F\(x\)+Wx，其中W是卷积操作，用来调整x维度的。

除了上面提到的两层残差学习单元，还有三层的残差学习单元，如下图所示：

![](../../.gitbook/assets/image%20%2817%29.png)

两种结构分别针对ResNet34（左图）和ResNet50/101/152（右图），其目的主要就是为了降低参数的数目。左图是两个3x3x256的卷积，参数数目: 3x3x256x256x2 = 1179648，右图是第一个1x1的卷积把256维通道降到64维，然后在最后通过1x1卷积恢复，整体上用的参数数目：1x1x256x64 + 3x3x64x64 + 1x1x64x256 = 69632，右图的参数数量比左图减少了16.94倍，因此，右图的主要目的就是为了减少参数量，从而减少计算量。 对于常规的ResNet，可以用于34层或者更少的网络中（左图）；对于更深的网络（如101层），则使用右图，_**其目的是减少计算和参数量。**_



可以注意到残差网络有这样几个特点：

* 网络较瘦，控制了参数数量；
* 存在明显层级，特征图个数逐层递进，保证输出特征表达能力
* 使用了较少的池化层，大量使用下采样，提高传播效率
* 4. 没有使用Dropout，利用BN和全局平均池化进行正则化，加快了训练速度
* 层数较高时减少了3x3卷积个数，并用1x1卷积控制了3x3卷积的输入输出特征图数量，称这种结构为“瓶颈”\(bottleneck\)。


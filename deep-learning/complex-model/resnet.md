# ResNet

 我们设深度网络中某隐含层为H（x）-x→F（x），如果可以假设多个非线性层组合可以近似于一个复杂函数，那么也同样可以假设隐含层的残差近似于某个复杂函数\[6\]。即那么我们可以将隐含层表示为H（x）=F（x）+ x。  
  
这样一来，我们就可以得到一种全新的残差结构单元，如图3所示。可以看到，残差单元的输出由多个卷积层级联的输出和输入元素间相加\(保证卷积层输出和输入元素维度相同\)，再经过ReLU激活后得到。将这种结构级联起来，就得到了残差网络。典型网络结构表1所示。  
![](https://pic2.zhimg.com/80/90e58f36fc1b0ae42443b69176cc2a75_hd.png)

可以注意到残差网络有这样几个特点：

* 网络较瘦，控制了参数数量；
* 存在明显层级，特征图个数逐层递进，保证输出特征表达能力
* 使用了较少的池化层，大量使用下采样，提高传播效率
* 4. 没有使用Dropout，利用BN和全局平均池化进行正则化，加快了训练速度
* 层数较高时减少了3x3卷积个数，并用1x1卷积控制了3x3卷积的输入输出特征图数量，称这种结构为“瓶颈”\(bottleneck\)。


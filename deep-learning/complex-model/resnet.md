# ResNet

 我们设深度网络中某隐含层为H（x）-x→F（x），如果可以假设多个非线性层组合可以近似于一个复杂函数，那么也同样可以假设隐含层的残差近似于某个复杂函数\[6\]。即那么我们可以将隐含层表示为H（x）=F（x）+ x。  
  
这样一来，我们就可以得到一种全新的残差结构单元，如图3所示。可以看到，残差单元的输出由多个卷积层级联的输出和输入元素间相加\(保证卷积层输出和输入元素维度相同\)，再经过ReLU激活后得到。将这种结构级联起来，就得到了残差网络。典型网络结构表1所示。  
![](https://pic2.zhimg.com/80/90e58f36fc1b0ae42443b69176cc2a75_hd.png)

在上图的残差网络结构图中，通过“shortcut connections（捷径连接）”的方式，直接把输入x传到输出作为初始结果，输出结果为H\(x\)=F\(x\)+x，当F\(x\)=0时，那么H\(x\)=x，也就是上面所提到的恒等映射。于是，ResNet相当于将学习目标改变了，不再是学习一个完整的输出，而是目标值H\(X\)和x的差值，也就是所谓的残差F\(x\) := H\(x\)-x，因此，后面的训练目标就是要将残差结果逼近于0，使到随着网络加深，准确率不下降。

可以注意到残差网络有这样几个特点：

* 网络较瘦，控制了参数数量；
* 存在明显层级，特征图个数逐层递进，保证输出特征表达能力
* 使用了较少的池化层，大量使用下采样，提高传播效率
* 4. 没有使用Dropout，利用BN和全局平均池化进行正则化，加快了训练速度
* 层数较高时减少了3x3卷积个数，并用1x1卷积控制了3x3卷积的输入输出特征图数量，称这种结构为“瓶颈”\(bottleneck\)。

